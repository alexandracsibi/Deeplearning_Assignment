{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Lj2PprLpbkD"
   },
   "source": [
    "# 0. Prequisite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ju03HrhIpV62"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "import torch_geometric as pyg\n",
    "import gradio as gr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjZZXCumpfqN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rrMmUBjbpfAC"
   },
   "outputs": [],
   "source": [
    "API_KEY = \"ad6669df-65b6-45f9-8e02-7ba74e788acd\"\n",
    "# API_KEY = \"c89e2d9e-94b2-4b84-8d22-bb525e63b73b\"\n",
    "\n",
    "params = {\"page_number\": 0}\n",
    "\n",
    "# Create a dictionary with HTTP headers\n",
    "headers = {\n",
    "    'Authorization': API_KEY,\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "# API endpoints\n",
    "url_gda = \"https://api.disgenet.com/api/v1/gda/summary\"\n",
    "url_disease = \"https://api.disgenet.com/api/v1/entity/disease\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bmmsIU8JwXGY"
   },
   "outputs": [],
   "source": [
    "# Function to handle API requests with rate-limiting handling\n",
    "def make_request(url, params, headers):\n",
    "    retries = 0\n",
    "    while retries < 5:\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "            # If rate-limited (HTTP 429), retry after waiting\n",
    "            if response.status_code == 429:\n",
    "                wait_time = int(response.headers.get('x-rate-limit-retry-after-seconds', 60))\n",
    "                print(f\"Rate limit exceeded. Waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                return response  # Return response if successful or error other than 429\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "\n",
    "    return None  # Return None if retries are exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KA-Rsx0AwhJ6"
   },
   "outputs": [],
   "source": [
    "def get_disease_ids(disease_type):\n",
    "    disease_ids = []\n",
    "    params['disease_free_text_search_string'] = disease_type\n",
    "\n",
    "    for page in range(100):\n",
    "      params['page_number'] = str(page)\n",
    "      response_disease = make_request(url_disease, params, headers)\n",
    "      if response_disease and response_disease.ok:\n",
    "          response_disease_json = response_disease.json()\n",
    "          data = response_disease_json.get(\"payload\", [])\n",
    "          for item in data:\n",
    "              for code_info in item.get(\"diseaseCodes\", []):\n",
    "                if code_info.get(\"vocabulary\") == \"MONDO\":\n",
    "                  disease_ids.append(f'MONDO_{code_info.get(\"code\")}')\n",
    "      else:\n",
    "          print(f\"Failed to fetch data for page {page}. Status code: {response_disease_json.status_code}\")\n",
    "          break\n",
    "    return list(set(disease_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6Eis2F6DzMn5"
   },
   "outputs": [],
   "source": [
    "def download_gda(disease_ids):\n",
    "    gda_data = []\n",
    "    params['disease'] = disease_ids\n",
    "\n",
    "    for page in range(100):\n",
    "        params['page_number'] = str(page)  # Different pages\n",
    "        response_gda = make_request(url_gda, params, headers)\n",
    "        if response_gda and response_gda.ok:\n",
    "            response_json = response_gda.json()\n",
    "            data = response_json.get(\"payload\", [])\n",
    "            gda_data.extend(data)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for page {page}. Status code: {response_json.status_code}\")\n",
    "            break  # If no more page or error\n",
    "    return gda_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "41BEp-MNzQaO"
   },
   "outputs": [],
   "source": [
    "def download_all_gda(ids, chunk_size=100):\n",
    "    all_data = []\n",
    "    for i in range(0, len(ids), chunk_size):\n",
    "        ids_chunk = ids[i:i + chunk_size]\n",
    "        ids_string = '\"' + ', '.join(ids_chunk) + '\"'\n",
    "        chunk_data = download_gda(ids_string)\n",
    "        all_data.extend(chunk_data)\n",
    "        print(f\"Downloaded the {i}. chunk\")\n",
    "    df_gda = pd.DataFrame(all_data)\n",
    "    df_gda.to_csv('GDA_df_raw.csv', index=False)\n",
    "    print(f\"All data saved to GDA_df_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o94-GA9pzT6a",
    "outputId": "2262f11c-f8a3-4f09-e197-3122b926a16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Waiting 8 seconds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5433"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_ids = get_disease_ids(\"disorder\")\n",
    "len(disease_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d7cTQ8OPzVlb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e2e6a75b-6544-4df1-c1ea-cfd386fd7724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Waiting 3 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 0. chunk\n",
      "Rate limit exceeded. Waiting 17 seconds...\n",
      "Rate limit exceeded. Waiting 11 seconds...\n",
      "Downloaded the 100. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 11 seconds...\n",
      "Downloaded the 200. chunk\n",
      "Rate limit exceeded. Waiting 19 seconds...\n",
      "Rate limit exceeded. Waiting 14 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 300. chunk\n",
      "Rate limit exceeded. Waiting 17 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 14 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 400. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 12 seconds...\n",
      "Downloaded the 500. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 8 seconds...\n",
      "Downloaded the 600. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 12 seconds...\n",
      "Downloaded the 700. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 11 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 800. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 13 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 900. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 10 seconds...\n",
      "Downloaded the 1000. chunk\n",
      "Rate limit exceeded. Waiting 19 seconds...\n",
      "Rate limit exceeded. Waiting 9 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 1100. chunk\n",
      "Rate limit exceeded. Waiting 17 seconds...\n",
      "Rate limit exceeded. Waiting 12 seconds...\n",
      "Downloaded the 1200. chunk\n",
      "Rate limit exceeded. Waiting 19 seconds...\n",
      "Rate limit exceeded. Waiting 16 seconds...\n",
      "Downloaded the 1300. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 11 seconds...\n",
      "Downloaded the 1400. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 11 seconds...\n",
      "Downloaded the 1500. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 10 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 1600. chunk\n",
      "Rate limit exceeded. Waiting 17 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Rate limit exceeded. Waiting 8 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 1700. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 14 seconds...\n",
      "Rate limit exceeded. Waiting 0 seconds...\n",
      "Downloaded the 1800. chunk\n",
      "Rate limit exceeded. Waiting 18 seconds...\n",
      "Rate limit exceeded. Waiting 10 seconds...\n",
      "Downloaded the 1900. chunk\n",
      "All data saved to GDA_df_raw.csv\n"
     ]
    }
   ],
   "source": [
    "download_all_gda(disease_ids[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re3dy4FCXSvN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUyX9kar9ZH5",
    "outputId": "16dd3ad3-bb7e-44b5-a906-93b21a2a6ff0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-c37a80c4584b>:2: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  GDA_disorder_df = pd.read_csv('GDA_df_disorder_raw.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 33127 entries, 0 to 33857\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   assocID                                         33127 non-null  int64  \n",
      " 1   symbolOfGene                                    33127 non-null  object \n",
      " 2   geneNcbiID                                      33127 non-null  int64  \n",
      " 3   geneEnsemblIDs                                  31908 non-null  object \n",
      " 4   geneNcbiType                                    33127 non-null  object \n",
      " 5   geneDSI                                         33127 non-null  float64\n",
      " 6   geneDPI                                         33127 non-null  float64\n",
      " 7   genepLI                                         29127 non-null  float64\n",
      " 8   geneProteinStrIDs                               31231 non-null  object \n",
      " 9   geneProteinClassIDs                             18322 non-null  object \n",
      " 10  geneProteinClassNames                           18322 non-null  object \n",
      " 11  diseaseVocabularies                             33127 non-null  object \n",
      " 12  diseaseName                                     33127 non-null  object \n",
      " 13  diseaseType                                     33127 non-null  object \n",
      " 14  diseaseUMLSCUI                                  33127 non-null  object \n",
      " 15  diseaseClasses_MSH                              31309 non-null  object \n",
      " 16  diseaseClasses_UMLS_ST                          33127 non-null  object \n",
      " 17  diseaseClasses_DO                               27623 non-null  object \n",
      " 18  diseaseClasses_HPO                              16368 non-null  object \n",
      " 19  numCTsupportingAssociation                      33127 non-null  int64  \n",
      " 20  chemicalsIncludedInEvidence                     0 non-null      float64\n",
      " 21  numberPmidsWithChemsIncludedInEvidenceBySource  7122 non-null   object \n",
      " 22  score                                           33127 non-null  float64\n",
      " 23  yearInitial                                     21267 non-null  float64\n",
      " 24  yearFinal                                       21267 non-null  float64\n",
      " 25  el                                              401 non-null    object \n",
      " 26  ei                                              28279 non-null  float64\n",
      " 27  numPMIDs                                        18996 non-null  float64\n",
      "dtypes: float64(9), int64(3), object(16)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "GDA_cancer_df = pd.read_csv('GDA_df_cancer_raw.csv')\n",
    "GDA_disorder_df = pd.read_csv('GDA_df_disorder_raw.csv')\n",
    "GDA_df = pd.concat([GDA_cancer_df, GDA_disorder_df]).drop_duplicates(subset='assocID', keep='first')\n",
    "GDA_df = GDA_df.map(lambda x: np.nan if x == '[]' else x)\n",
    "GDA_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "Tg7zKA8RXulo"
   },
   "outputs": [],
   "source": [
    "GDA_df = GDA_df[[\n",
    "    'geneNcbiID',\n",
    "    'geneDSI',\n",
    "    'geneDPI',\n",
    "    'geneNcbiType',\n",
    "    'diseaseUMLSCUI',\n",
    "    'diseaseClasses_MSH',\n",
    "    'diseaseClasses_UMLS_ST',\n",
    "    'diseaseType',\n",
    "    'assocID',\n",
    "    'score'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "CpQNIkUPYGyb"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding geneNcbiType\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_geneNcbiType = enc.fit_transform(GDA_df[['geneNcbiType']])\n",
    "columns = ['geneType_' + col.split('_')[-1] for col in enc.get_feature_names_out(['geneNcbiType'])]\n",
    "encoded_df = pd.DataFrame(encoded_geneNcbiType, columns=columns)\n",
    "GDA_df = pd.concat([GDA_df.reset_index(drop=True), encoded_df], axis=1).drop('geneNcbiType', axis=1)\n",
    "\n",
    "# One-hot encoding diseaseType\n",
    "encoded_diseaseType = enc.fit_transform(GDA_df[['diseaseType']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_diseaseType,\n",
    "    columns=enc.get_feature_names_out(['diseaseType'])\n",
    ")\n",
    "GDA_df = pd.concat([GDA_df.reset_index(drop=True), encoded_df], axis=1).drop('diseaseType', axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "GDA_df['diseaseUMLSCUI'] = label_encoder.fit_transform(GDA_df['diseaseUMLSCUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "EtmQFmBnZt2X"
   },
   "outputs": [],
   "source": [
    "# Keep only IDs for simplicity\n",
    "def clean_classes(entry):\n",
    "    if isinstance(entry, (str, bytes)):\n",
    "        return [match.strip() for match in re.findall(r'\\((.*?)\\)', entry)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "GDA_df['diseaseClasses_UMLS_ST'] = GDA_df['diseaseClasses_UMLS_ST'].apply(clean_classes)\n",
    "GDA_df['diseaseClasses_MSH'] = GDA_df['diseaseClasses_MSH'].apply(clean_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "xB0c-KtMZvCL"
   },
   "outputs": [],
   "source": [
    "# Combine the two lists into a new column for handling potential missing values in diseaseClasses_MSH\n",
    "GDA_df['diseaseClass'] = GDA_df.apply(\n",
    "    lambda row: list(set(row['diseaseClasses_UMLS_ST'] + row['diseaseClasses_MSH'])),\n",
    "    axis=1\n",
    ")\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_diseaseClass = mlb.fit_transform(GDA_df['diseaseClass'])\n",
    "enc_df = pd.DataFrame(encoded_diseaseClass, columns=['diseaseClass_' + cols for cols in mlb.classes_])\n",
    "GDA_df = pd.concat([GDA_df.reset_index(drop=True), enc_df], axis=1)\n",
    "\n",
    "GDA_df = GDA_df.drop(['diseaseClasses_UMLS_ST', 'diseaseClasses_MSH', 'diseaseClass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "tbtCbVVqaDS2"
   },
   "outputs": [],
   "source": [
    "GDA_df.rename(columns={'geneNcbiID': 'geneID', 'diseaseUMLSCUI': 'diseaseID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbjglXniac4u",
    "outputId": "a391e683-41c6-49db-bd4c-178b613a3ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique gene IDs: 9052\n",
      "Number of unique disease IDs: 2225\n",
      "Number of unique assocIDs: 33127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique gene IDs: {len(GDA_df['geneID'].unique())}\")\n",
    "print(f\"Number of unique disease IDs: {len(GDA_df['diseaseID'].unique())}\")\n",
    "print(f\"Number of unique assocIDs: {len(GDA_df['assocID'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZYsifupath2"
   },
   "outputs": [],
   "source": [
    "GDA_df.to_csv('GDA_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U62r8Apwa9HH"
   },
   "source": [
    "# 3. Graph Data Preparation\n",
    "Creating Homogeneous Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pQYBqvoc_Jhn"
   },
   "outputs": [],
   "source": [
    "def homogeneous_node_features(df):\n",
    "    '''Preprocess and construct node features for genes and diseases for homogeneous graph'''\n",
    "    # Extract unique rows for genes and diseases\n",
    "    gene_rows = df[\n",
    "        ['geneID', 'geneDSI', 'geneDPI'] +\n",
    "        [col for col in df.columns if col.startswith('geneType')]]\n",
    "    gene_rows = gene_rows.drop_duplicates(subset=['geneID']).drop(columns=['geneID'])\n",
    "\n",
    "    disease_rows = df[\n",
    "        ['diseaseID'] +\n",
    "        [col for col in df.columns if col.startswith('diseaseClass')] +\n",
    "        [col for col in df.columns if col.startswith('diseaseType')]]\n",
    "    disease_rows = disease_rows.drop_duplicates(subset=['diseaseID']).drop(columns=['diseaseID'])\n",
    "\n",
    "    # Fill missing columns with zeros where needed\n",
    "    gene_rows = gene_rows.assign(**{col: 0 for col in disease_rows.columns if col not in gene_rows.columns})\n",
    "    disease_rows = disease_rows.assign(**{col: 0 for col in gene_rows.columns if col not in disease_rows.columns})\n",
    "\n",
    "    # Convert features to numpy arrays and add node type indicator\n",
    "    gene_features = np.hstack([gene_rows.values, np.ones((gene_rows.shape[0], 1))]) # 1 indicates gene\n",
    "    disease_features = np.hstack([disease_rows.values, np.zeros((disease_rows.shape[0], 1))]) # 0 indicates disease\n",
    "\n",
    "    # Combine gene and disease features into a single matrix and return as tensor\n",
    "    node_features = np.vstack([gene_features, disease_features])\n",
    "\n",
    "    return torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yvYP1eGmDlFI"
   },
   "outputs": [],
   "source": [
    "def prepare_homogeneous_graph(df):\n",
    "    '''Prepare a homogeneous graph for PyTorch Geometric'''\n",
    "    # Map IDs to separate index ranges\n",
    "    unique_gene_ids = df['geneID'].unique()\n",
    "    unique_disease_ids = df['diseaseID'].unique()\n",
    "\n",
    "    # geneIds 0 to len(unique_gene_ids) and diseaseIds len(unique_gene_ids) to len(unique_gene_ids) + len(unique_disease_ids)\n",
    "    gene_id_to_idx = {id: idx for idx, id in enumerate(unique_gene_ids)}\n",
    "    disease_id_to_idx = {id: idx + len(unique_gene_ids) for idx, id in enumerate(unique_disease_ids)}\n",
    "\n",
    "    df['geneID'] = df['geneID'].map(gene_id_to_idx)\n",
    "    df['diseaseID'] = df['diseaseID'].map(disease_id_to_idx)\n",
    "\n",
    "    # Construct node features\n",
    "    node_features = homogeneous_node_features(df)\n",
    "\n",
    "    # Create edge indices\n",
    "    edge_index = torch.tensor(np.array([df['geneID'].values, df['diseaseID'].values]), dtype=torch.long)\n",
    "    edge_index = pyg.utils.to_undirected(edge_index) # Make edges bidirectional\n",
    "\n",
    "    # Homogeneous Graph\n",
    "    graph_data = pyg.data.Data(\n",
    "        x = node_features,\n",
    "        edge_index = edge_index,\n",
    "    )\n",
    "\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqurBCGm8FrK"
   },
   "source": [
    "# 4. Models\n",
    "- GCN_DP\n",
    "- GCN_MLP\n",
    "- GraphSAGE_MLP\n",
    "- GIN_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4BgvFY0ZviJG"
   },
   "outputs": [],
   "source": [
    "class GCN_DP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        return (src * dst).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jq-R6bFYyY3E"
   },
   "outputs": [],
   "source": [
    "class GCN_MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(output_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        edge_features = torch.cat([src, dst], dim=-1)\n",
    "        return self.mlp(edge_features).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L7TvEH59QGvq"
   },
   "outputs": [],
   "source": [
    "class GraphSAGE_MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg.nn.SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = pyg.nn.SAGEConv(hidden_dim, output_dim)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(output_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        edge_features = torch.cat([src, dst], dim=-1)\n",
    "        return self.mlp(edge_features).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DPPT5pMVyplq"
   },
   "outputs": [],
   "source": [
    "class GIN_MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg.nn.GINConv(torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 2 * hidden_dim),\n",
    "            torch.nn.BatchNorm1d(2 * hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        ), train_eps=True)\n",
    "        self.conv2 = pyg.nn.GINConv(torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 2 * hidden_dim),\n",
    "            torch.nn.BatchNorm1d(2 * hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2 * hidden_dim, output_dim)\n",
    "        ), train_eps=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define MLP for decoding\n",
    "        self.mlp_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(output_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        edge_features = torch.cat([src, dst], dim=-1)\n",
    "        return self.mlp_decoder(edge_features).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWtU220OhwqH"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AVJ9IXG9Roa"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu', save_path='best_model.pth'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.save_path = save_path\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train_epoch(self, data):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Encode node embeddings\n",
    "        data = data.to(self.device)\n",
    "        z = self.model.encode(data.x, data.edge_index)\n",
    "\n",
    "        # Decode edge predictions\n",
    "        pos_out = self.model.decode(z, data.pos_edge_label_index).view(-1)\n",
    "        neg_out = self.model.decode(z, data.neg_edge_label_index).view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        pos_loss = self.criterion(pos_out, data.pos_edge_label.float())\n",
    "        neg_loss = self.criterion(neg_out, data.neg_edge_label.float())\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), pos_loss.item(), neg_loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data):\n",
    "        self.model.eval()\n",
    "\n",
    "        data = data.to(self.device)\n",
    "        z = self.model.encode(data.x, data.edge_index)\n",
    "\n",
    "        pos_out = self.model.decode(z, data.pos_edge_label_index).view(-1)\n",
    "        neg_out = self.model.decode(z, data.neg_edge_label_index).view(-1)\n",
    "        scores = torch.cat([pos_out, neg_out]).sigmoid()\n",
    "        labels = torch.cat([data.pos_edge_label, data.neg_edge_label])\n",
    "\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred_probs = scores.cpu().numpy()\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_pred_probs)\n",
    "\n",
    "        # Dynamically evaluate performance across thresholds\n",
    "        best_f1, best_threshold, best_cm = 0, 0, None\n",
    "\n",
    "        for threshold in [i / 100 for i in range(1, 100)]:\n",
    "            y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            if f1  > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_cm = cm\n",
    "\n",
    "        return {\n",
    "            'f1': best_f1,\n",
    "            'auc': auc,\n",
    "            'threshold': best_threshold,\n",
    "            'cm': best_cm\n",
    "        }\n",
    "\n",
    "    def fit(self, train_data, val_data, num_epochs=100, early_stopping_patience=10):\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train_loss, pos_loss, neg_loss = self.train_epoch(train_data)\n",
    "            val_metric = self.evaluate(val_data)\n",
    "\n",
    "            # Save the best model\n",
    "            if val_metric['f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metric['f1']\n",
    "                patience_counter = 0\n",
    "                torch.save({'model_state_dict': self.model.state_dict(),\n",
    "                'best_threshold': val_metric['threshold']}, self.save_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "            self.scheduler.step()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{'Epoch':<6} {'Train Loss':<12} {'Pos Loss':<10} {'Neg Loss':<10} {'Val F1':<8} {'Val AUC':<9} {'Threshold':<10}\")\n",
    "                print(f\"  {epoch:<6} {train_loss:<12.4f} {pos_loss:<10.4f} {neg_loss:<10.4f} {val_metric['f1']:<8.4f} {val_metric['auc']:<9.4f} {val_metric['threshold']:<10.2f}\")\n",
    "                print(f\"{'=' * 70}\")\n",
    "                print(f\"Confusion Matrix:\\n{val_metric['cm']}\")\n",
    "                print(f\"{'=' * 70}\")\n",
    "\n",
    "    def test(self, test_data):\n",
    "        checkpoint = torch.load(self.save_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_threshold = checkpoint['best_threshold']\n",
    "        test_metric = self.evaluate(test_data)\n",
    "        print(f\"Test F1: {test_metric['f1']:.4f}\")\n",
    "        print(f\"Test AUC: {test_metric['auc']:.4f}\")\n",
    "        print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "        print(f\"Confusion Matrix:\\n{test_metric['cm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oilIwLUDmYQC"
   },
   "source": [
    "# Initialization and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIx6beMaF_1P",
    "outputId": "89585675-a6bc-4d25-b16e-44230e1b0816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: Data(x=[11277, 48], edge_index=[2, 53006], pos_edge_label=[26503], pos_edge_label_index=[2, 26503], neg_edge_label=[26503], neg_edge_label_index=[2, 26503])\n",
      "Val data: Data(x=[11277, 48], edge_index=[2, 53006], pos_edge_label=[3312], pos_edge_label_index=[2, 3312], neg_edge_label=[3312], neg_edge_label_index=[2, 3312])\n",
      "Test data: Data(x=[11277, 48], edge_index=[2, 59630], pos_edge_label=[3312], pos_edge_label_index=[2, 3312], neg_edge_label=[3312], neg_edge_label_index=[2, 3312])\n"
     ]
    }
   ],
   "source": [
    "GDA_df = pd.read_csv('GDA_df.csv')\n",
    "homogeneous_graph = prepare_homogeneous_graph(GDA_df)\n",
    "\n",
    "split = pyg.transforms.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    split_labels=True)\n",
    "train_data, val_data, test_data = split(homogeneous_graph)\n",
    "\n",
    "print(f'Train data: {train_data}')\n",
    "print(f'Val data: {val_data}')\n",
    "print(f'Test data: {test_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c8C13ckUFnBN"
   },
   "outputs": [],
   "source": [
    "input_dim = homogeneous_graph.num_node_features\n",
    "hidden_dim = 128\n",
    "output_dim = 64\n",
    "dropout = 0.2\n",
    "wd = 1e-4\n",
    "lr = 1e-3\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XTR5INmmzBj5"
   },
   "outputs": [],
   "source": [
    "model = GCN_DP(homogeneous_graph.num_node_features, hidden_dim, output_dim, dropout)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Pk7qcymhFv2P"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    save_path='test.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix9YS5FDs9q7"
   },
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfRAlrnfFyL3",
    "outputId": "2a860be5-f64e-4e22-b695-a2be8b61e50b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  5      1.2038       0.3052     0.8987     0.8067   0.8832    0.61      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2689  623]\n",
      " [ 652 2660]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  10     1.1812       0.3113     0.8699     0.8206   0.8992    0.61      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2772  540]\n",
      " [ 632 2680]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  15     1.1776       0.3026     0.8750     0.8293   0.9084    0.61      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2736  576]\n",
      " [ 558 2754]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  20     1.1476       0.2365     0.9111     0.8602   0.9343    0.63      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2748  564]\n",
      " [ 387 2925]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  25     1.1185       0.2249     0.8935     0.8691   0.9405    0.63      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2895  417]\n",
      " [ 446 2866]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  30     1.1067       0.2343     0.8724     0.8694   0.9404    0.62      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2892  420]\n",
      " [ 442 2870]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  35     1.1050       0.2402     0.8648     0.8702   0.9401    0.62      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2930  382]\n",
      " [ 467 2845]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  40     1.0917       0.2574     0.8342     0.8711   0.9350    0.60      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2876  436]\n",
      " [ 420 2892]]\n",
      "======================================================================\n",
      "Epoch  Train Loss   Pos Loss   Neg Loss   Val F1   Val AUC   Threshold \n",
      "  45     1.0708       0.2213     0.8495     0.8688   0.9290    0.62      \n",
      "======================================================================\n",
      "Confusion Matrix:\n",
      "[[2937  375]\n",
      " [ 480 2832]]\n",
      "======================================================================\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_data, val_data, num_epochs=num_epochs, early_stopping_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f15qqgEFzgZ",
    "outputId": "33a07bd4-fb70-45e6-8e89-12ba9975ad3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/2282156664.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.8777\n",
      "Test AUC: 0.9368\n",
      "Best Threshold: 0.60\n",
      "Confusion Matrix:\n",
      "[[2911  401]\n",
      " [ 408 2904]]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NMnw9HW6FJ7"
   },
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m9ru_gMGQTSE"
   },
   "outputs": [],
   "source": [
    "def fetch_gene_features(gene_id, api_key):\n",
    "    url = \"https://api.disgenet.com/api/v1/entity/gene\"\n",
    "    params = {\"gene_ncbi_id\": gene_id}\n",
    "    headers = {'Authorization': api_key, 'accept': 'application/json'}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.ok:\n",
    "          response_json = response.json()\n",
    "          data = response_json.get(\"payload\", [])\n",
    "          records = []\n",
    "          for item in data:\n",
    "              records.append({\n",
    "                  \"geneDSI\": item.get(\"dsi\", 0),\n",
    "                  \"geneDPI\": item.get(\"dpi\", 0),\n",
    "                  \"geneNcbiType\": item.get(\"ncbi_type\", \"unknown\"),\n",
    "              })\n",
    "          return pd.DataFrame(records)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Error fetching gene features: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9YmdQ9M4RSmZ"
   },
   "outputs": [],
   "source": [
    "def fetch_disease_features(disease_id, api_key):\n",
    "    url = \"https://api.disgenet.com/api/v1/entity/disease\"\n",
    "    params = {\"disease\": disease_id}\n",
    "    headers = {'Authorization': api_key, 'accept': 'application/json'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.ok:\n",
    "          response_json = response.json()\n",
    "          data = response_json.get(\"payload\", [])\n",
    "          records = []\n",
    "          for item in data:\n",
    "              records.append({\n",
    "                  \"diseaseType\": item.get(\"type\", \"unknown\"),\n",
    "                  \"diseaseClasses_MSH\": item.get(\"diseaseClasses_MSH\", \"unknown\"),\n",
    "                  \"diseaseClasses_UMLS_ST\": item.get(\"diseaseClasses_UMLS_ST\", \"unknown\"),\n",
    "              })\n",
    "\n",
    "          return pd.DataFrame(records)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Error fetching gene features: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SuhFuC3sW2-X"
   },
   "outputs": [],
   "source": [
    "def extract_parentheses_values(entry):\n",
    "    \"\"\"Extract values enclosed in parentheses from a string or list of strings.\"\"\"\n",
    "    if isinstance(entry, str):\n",
    "        return re.findall(r'\\((.*?)\\)', entry)\n",
    "    elif isinstance(entry, list):\n",
    "        result = []\n",
    "        for item in entry:\n",
    "            result.extend(re.findall(r'\\((.*?)\\)', item))\n",
    "        return result\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jQ9X0UXuZrT7"
   },
   "outputs": [],
   "source": [
    "def process_gene_disease_features(df, disease_features, gene_features):\n",
    "\n",
    "    gene_columns = ['geneDSI', 'geneDPI'] + [col for col in df.columns if col.startswith('geneType')]\n",
    "    disease_columns = [col for col in df.columns if col.startswith('diseaseClass') or col.startswith('diseaseType')]\n",
    "\n",
    "    all_columns = gene_columns + disease_columns + ['nodetype']\n",
    "    base_template = {col: 0 for col in all_columns}\n",
    "\n",
    "    # Process the gene features\n",
    "    processed_gene_features = base_template.copy()\n",
    "    processed_gene_features['geneDSI'] = gene_features['geneDSI'].iloc[0]\n",
    "    processed_gene_features['geneDPI'] = gene_features['geneDPI'].iloc[0]\n",
    "\n",
    "    # Dynamically add the one-hot encoded gene type\n",
    "    gene_type_column = f\"geneType_{gene_features['geneNcbiType'].iloc[0]}\"\n",
    "    if gene_type_column in processed_gene_features:\n",
    "        processed_gene_features[gene_type_column] = 1\n",
    "\n",
    "    processed_gene_features['nodetype'] = 1\n",
    "\n",
    "    processed_disease_features = base_template.copy()\n",
    "\n",
    "    # Dynamically add the one-hot encoded disease type\n",
    "    disease_type_column = f\"diseaseType_{disease_features['diseaseType'].iloc[0]}\"\n",
    "    if disease_type_column in processed_disease_features:\n",
    "        processed_disease_features[disease_type_column] = 1\n",
    "\n",
    "    msh_classes = extract_parentheses_values(disease_features['diseaseClasses_MSH'].iloc[0])\n",
    "    umls_classes = extract_parentheses_values(disease_features['diseaseClasses_UMLS_ST'].iloc[0])\n",
    "    disease_classes = set(msh_classes + umls_classes)\n",
    "\n",
    "    for disease_class in disease_classes:\n",
    "        disease_class_column = f\"diseaseClass_{disease_class}\"\n",
    "        if disease_class_column in processed_disease_features:\n",
    "            processed_disease_features[disease_class_column] = 1\n",
    "\n",
    "    # Add the nodetype indicator for diseases\n",
    "    processed_disease_features['nodetype'] = 0  # 0 for diseases\n",
    "\n",
    "    # Convert processed features to DataFrames for consistency\n",
    "    processed_gene_df = pd.DataFrame([processed_gene_features])\n",
    "    processed_disease_df = pd.DataFrame([processed_disease_features])\n",
    "\n",
    "    gene_node_feature = torch.tensor(processed_gene_df.values, dtype=torch.float)\n",
    "    disease_node_feature = torch.tensor(processed_disease_df.values, dtype=torch.float)\n",
    "\n",
    "    return gene_node_feature, disease_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "soJwRxF1mEGB"
   },
   "outputs": [],
   "source": [
    "def check_and_assign_ids(graph_data, gene_node_feature, disease_node_feature):\n",
    "    \"\"\"\n",
    "    Check if the provided gene and disease features exist in the graph.\n",
    "    If not, add them to the graph, ensuring gene nodes are added after the current gene nodes\n",
    "    and disease nodes after the current disease nodes. Assign IDs accordingly.\n",
    "    \"\"\"\n",
    "    node_features = graph_data.x\n",
    "    nodetype_indicator = node_features[:, -1]\n",
    "\n",
    "    gene_features = node_features[nodetype_indicator == 1]\n",
    "    disease_features = node_features[nodetype_indicator == 0]\n",
    "\n",
    "    # Check for gene node existence\n",
    "    gene_id = None\n",
    "    if (gene_features == gene_node_feature).all(dim=1).any():\n",
    "        gene_id = torch.where((gene_features == gene_node_feature).all(dim=1))[0][0].item()\n",
    "    else:\n",
    "        gene_id = gene_features.size(0)\n",
    "        node_features = torch.cat([node_features, gene_node_feature], dim=0)\n",
    "\n",
    "    # Check for disease node existence\n",
    "    disease_id = None\n",
    "    if (disease_features == disease_node_feature).all(dim=1).any():\n",
    "        disease_id = torch.where((disease_features == disease_node_feature).all(dim=1))[0][0].item() + gene_features.size(0)\n",
    "    else:\n",
    "        disease_id = node_features.size(0)  # Append disease after all existing nodes\n",
    "        node_features = torch.cat([node_features, disease_node_feature], dim=0)\n",
    "\n",
    "    graph_data.x = node_features\n",
    "\n",
    "    return graph_data, gene_id, disease_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "2wNvM5NQCsvd",
    "outputId": "b41478e4-8625-4fa9-e558-46d506c47b81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/4127009907.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"GIN_MLP.pth\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GIN_MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     graph_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGIN_MLP.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGIN_MLP\u001b[49m(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GIN_MLP' is not defined"
     ]
    }
   ],
   "source": [
    "# Define global variables\n",
    "GDA_df = pd.read_csv('GDA_df.csv')\n",
    "df = GDA_df.copy()\n",
    "\n",
    "# Load the graph and model\n",
    "with open(\"graph_data.pkl\", \"rb\") as f:\n",
    "    graph_data = pickle.load(f)\n",
    "\n",
    "checkpoint = torch.load(\"GIN_MLP.pth\")\n",
    "model = GIN_MLP(input_dim=48, hidden_dim=128, output_dim=64)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "best_threshold = checkpoint['best_threshold']\n",
    "\n",
    "# Define API Key (You should replace it with a valid one)\n",
    "API_KEY = \"ad6669df-65b6-45f9-8e02-7ba74e788acd\"\n",
    "\n",
    "# Prediction function\n",
    "def gradio_predict(gene_id: int, disease_id: str):\n",
    "    try:\n",
    "        # Strip whitespace from inputs\n",
    "        gene_id = gene_id.strip()\n",
    "        disease_id = disease_id.strip()\n",
    "\n",
    "        # Fetch features for the gene and disease\n",
    "        gene_features = fetch_gene_features(gene_id, API_KEY)\n",
    "        disease_features = fetch_disease_features(disease_id, API_KEY)\n",
    "\n",
    "        # Process the fetched features into node features\n",
    "        processed_gene_feature, processed_disease_feature = process_gene_disease_features(\n",
    "            df, disease_features, gene_features\n",
    "        )\n",
    "\n",
    "        # Check and assign IDs in the graph\n",
    "        updated_graph, gene_idx, disease_idx = check_and_assign_ids(\n",
    "            graph_data, processed_gene_feature, processed_disease_feature\n",
    "        )\n",
    "\n",
    "        # Prepare edge indices for prediction\n",
    "        edge_label_index = torch.tensor([[gene_idx], [disease_idx]], dtype=torch.long)\n",
    "\n",
    "        # Predict association\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(updated_graph.x, updated_graph.edge_index)  # Encode node embeddings\n",
    "            raw_score = model.decode(z, edge_label_index).sigmoid().item()  # Get the sigmoid of the predicted score\n",
    "\n",
    "        # Apply the best threshold\n",
    "        prediction = 1 if raw_score >= best_threshold else 0\n",
    "\n",
    "        # Return results\n",
    "        result = f\"Prediction: {'Exists' if prediction == 1 else 'Does not exist'}\"\n",
    "        raw_score_str = f\"Raw Score: {raw_score:.4f}\"\n",
    "        return result, raw_score_str\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\"\n",
    "\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Gene ID, the Entrez Id from Disgenet (e.g., 7124)\", placeholder=\"Enter Gene ID here\"),\n",
    "        gr.Textbox(label=\"Disease ID (e.g., MONDO_0000728)\", placeholder=\"Enter Disease ID here\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Prediction\"),\n",
    "        gr.Textbox(label=\"Raw Score\")\n",
    "    ],\n",
    "    title=\"Gene-Disease Association Prediction\",\n",
    "    description=\"Enter Gene and Disease IDs to predict the association using the trained model.\"\n",
    ")\n",
    "\n",
    "# Launch Gradio interface\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PjZZXCumpfqN",
    "t-Upkxc97_2C",
    "MqurBCGm8FrK",
    "8NMnw9HW6FJ7"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
